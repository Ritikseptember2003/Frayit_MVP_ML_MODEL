{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsfad3Ogr/Ppl2z9JH7G7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritikseptember2003/Frayit_MVP_ML_MODEL/blob/main/Enhanced_toxicity_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eai5gBTl1cAr",
        "outputId": "27efb8bf-db75-4e9f-ce3e-d513f69ab6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Run this in your first Colab cell\n",
        "!pip install pandas numpy scikit-learn xgboost matplotlib seaborn nltk flask joblib requests\n",
        "!pip install --upgrade scikit-learn\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Please upload the jigsaw-toxic-comment-classification-challenge.zip file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the dataset\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Extracting {filename}...\")\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('jigsaw_data/')\n",
        "\n",
        "    # If train.csv is also zipped inside\n",
        "    if os.path.exists('jigsaw_data/train.csv.zip'):\n",
        "        with zipfile.ZipFile('jigsaw_data/train.csv.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('jigsaw_data/')\n",
        "\n",
        "print(\"âœ… Dataset uploaded and extracted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "fxG8saCL1lQs",
        "outputId": "0669177f-c4be-40f8-a9fd-079e71fc39bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the jigsaw-toxic-comment-classification-challenge.zip file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-34aea334-38a2-4fb9-9953-7c0004953e80\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-34aea334-38a2-4fb9-9953-7c0004953e80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving jigsaw-toxic-comment-classification-challenge.zip to jigsaw-toxic-comment-classification-challenge.zip\n",
            "Extracting jigsaw-toxic-comment-classification-challenge.zip...\n",
            "âœ… Dataset uploaded and extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the Dataset"
      ],
      "metadata": {
        "id": "aAP00it325ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "zip_files = [f for f in os.listdir('.') if f.endswith('.zip')]\n",
        "print(f\"Found zip files: {zip_files}\")\n",
        "\n",
        "# Extract the first zip file found\n",
        "if zip_files:\n",
        "    zip_filename = zip_files[0]\n",
        "    print(f\"Extracting {zip_filename}...\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('dataset/')\n",
        "\n",
        "    print(\"âœ… Extraction complete!\")\n",
        "\n",
        "    # List extracted files\n",
        "    for root, dirs, files in os.walk('dataset/'):\n",
        "        for file in files:\n",
        "            print(f\"Found: {os.path.join(root, file)}\")\n",
        "else:\n",
        "    print(\"âŒ No zip file found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhsR6Kga29Sz",
        "outputId": "063eeab1-a42f-4326-f280-f546434c787c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found zip files: ['jigsaw-toxic-comment-classification-challenge.zip']\n",
            "Extracting jigsaw-toxic-comment-classification-challenge.zip...\n",
            "âœ… Extraction complete!\n",
            "Found: dataset/train.csv.zip\n",
            "Found: dataset/test_labels.csv.zip\n",
            "Found: dataset/sample_submission.csv.zip\n",
            "Found: dataset/test.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the train.csv File"
      ],
      "metadata": {
        "id": "5c9V3Hyk2_Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look for train.csv file\n",
        "train_csv_path = None\n",
        "\n",
        "for root, dirs, files in os.walk('.'):\n",
        "    for file in files:\n",
        "        if file == 'train.csv':\n",
        "            train_csv_path = os.path.join(root, file)\n",
        "            print(f\"âœ… Found train.csv at: {train_csv_path}\")\n",
        "            break\n",
        "    if train_csv_path:\n",
        "        break\n",
        "\n",
        "# If train.csv is also zipped, extract it\n",
        "if not train_csv_path:\n",
        "    for root, dirs, files in os.walk('.'):\n",
        "        for file in files:\n",
        "            if file == 'train.csv.zip':\n",
        "                print(f\"Found train.csv.zip, extracting...\")\n",
        "                with zipfile.ZipFile(os.path.join(root, file), 'r') as zip_ref:\n",
        "                    zip_ref.extractall('dataset/')\n",
        "\n",
        "                # Look again for train.csv\n",
        "                for root2, dirs2, files2 in os.walk('dataset/'):\n",
        "                    for file2 in files2:\n",
        "                        if file2 == 'train.csv':\n",
        "                            train_csv_path = os.path.join(root2, file2)\n",
        "                            print(f\"âœ… Extracted and found train.csv at: {train_csv_path}\")\n",
        "                            break\n",
        "                break\n",
        "\n",
        "if train_csv_path:\n",
        "    print(f\"âœ… Dataset ready at: {train_csv_path}\")\n",
        "else:\n",
        "    print(\"âŒ Could not find train.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Jvge7Q3EW1",
        "outputId": "1d08b40d-efe4-48cd-8c0b-d644d9638329"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found train.csv at: ./jigsaw_data/train.csv\n",
            "âœ… Dataset ready at: ./jigsaw_data/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Cross Checking"
      ],
      "metadata": {
        "id": "xF0lq2vH3Lg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick check of the dataset\n",
        "if train_csv_path:\n",
        "    df = pd.read_csv(train_csv_path)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head(2))\n",
        "\n",
        "    # Check toxicity columns\n",
        "    toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    available_toxicity = [col for col in toxicity_cols if col in df.columns]\n",
        "    print(f\"\\nToxicity columns found: {available_toxicity}\")\n",
        "\n",
        "    if available_toxicity:\n",
        "        print(\"âœ… Dataset looks good!\")\n",
        "    else:\n",
        "        print(\"âŒ No toxicity columns found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IEIFrdB3O1j",
        "outputId": "87184c88-4349-4b00-8ec0-2196fd7d2517"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (159571, 8)\n",
            "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "\n",
            "First few rows:\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "\n",
            "Toxicity columns found: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "âœ… Dataset looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "e9PVN5qg_KXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLc4J95Taoi3",
        "outputId": "e8141579-7c5b-4f74-a7cc-927a07eb9997"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ENHANCED TOXICITY DETECTION MODEL WITH RL\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import requests\n",
        "import time, json, os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "import re\n",
        "\n",
        "# Enhanced preprocessing\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except:\n",
        "    STOPWORDS = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
        "\n",
        "# Flask for deployment\n",
        "try:\n",
        "    from flask import Flask, request, jsonify, render_template_string\n",
        "    # pyngrok for tunneling (required for Colab)\n",
        "    from pyngrok import ngrok\n",
        "    FLASK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASK_AVAILABLE = False\n",
        "    print(\"Flask not available. Install with: pip install flask pyngrok\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIG & ENHANCED PARAMETERS\n",
        "# ============================================\n",
        "MODEL_PATH = \"enhanced_toxicity_model.pkl\"\n",
        "RL_MODEL_PATH = \"rl_toxicity_agent.pkl\"\n",
        "REPORTS_PATH = \"reports.json\"\n",
        "FEEDBACK_PATH = \"feedback.csv\"\n",
        "PERFORMANCE_LOG = \"performance_log.json\"\n",
        "\n",
        "# Toxicity categories with severity weights\n",
        "TOXICITY_CATEGORIES = {\n",
        "    'toxic': {'weight': 1.0, 'description': 'General toxic behavior'},\n",
        "    'severe_toxic': {'weight': 2.0, 'description': 'Severely toxic content'},\n",
        "    'obscene': {'weight': 1.2, 'description': 'Obscene language'},\n",
        "    'threat': {'weight': 2.5, 'description': 'Threatening behavior'},\n",
        "    'insult': {'weight': 1.1, 'description': 'Insulting language'},\n",
        "    'identity_hate': {'weight': 2.0, 'description': 'Identity-based hate speech'}\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED TEXT PREPROCESSING\n",
        "# ============================================\n",
        "def clean_text(text):\n",
        "    \"\"\"Enhanced text cleaning and preprocessing\"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Handle contractions\n",
        "    contractions = {\n",
        "        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "        \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
        "        \"'m\": \" am\", \"u\": \"you\", \"ur\": \"your\", \"ppl\": \"people\"\n",
        "    }\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text = text.replace(contraction, expansion)\n",
        "\n",
        "    # Remove excessive punctuation but keep some for context\n",
        "    text = re.sub(r'[!]{2,}', '!', text)\n",
        "    text = re.sub(r'[?]{2,}', '?', text)\n",
        "    text = re.sub(r'[.]{3,}', '...', text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def extract_features(text):\n",
        "    \"\"\"Extract additional features from text\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "\n",
        "    return {\n",
        "        'length': len(text),\n",
        "        'word_count': len(text.split()),\n",
        "        'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),\n",
        "        'exclamation_count': text.count('!'),\n",
        "        'question_count': text.count('?'),\n",
        "        'profanity_indicators': len(re.findall(r'\\b(damn|hell|crap|stupid|idiot)\\b', text.lower())),\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# REINFORCEMENT LEARNING AGENT\n",
        "# ============================================\n",
        "class ToxicityRLAgent:\n",
        "    def __init__(self, state_size=10, action_size=6, learning_rate=0.001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size  # 6 toxicity categories\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        # Q-table for simple Q-learning (can be upgraded to neural network)\n",
        "        self.q_table = {}\n",
        "        self.reward_history = []\n",
        "\n",
        "    def get_state(self, text_features, prediction_confidence):\n",
        "        \"\"\"Convert text features and prediction confidence to state\"\"\"\n",
        "        state = tuple([\n",
        "            min(int(text_features.get('length', 0) / 10), 9),\n",
        "            min(int(text_features.get('word_count', 0) / 5), 9),\n",
        "            min(int(text_features.get('caps_ratio', 0) * 10), 9),\n",
        "            min(text_features.get('exclamation_count', 0), 9),\n",
        "            min(text_features.get('question_count', 0), 9),\n",
        "            min(text_features.get('profanity_indicators', 0), 9),\n",
        "            min(int(prediction_confidence * 10), 9),\n",
        "            0, 0, 0  # padding for fixed state size\n",
        "        ][:self.state_size])\n",
        "        return state\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Choose action based on epsilon-greedy policy\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = np.zeros(self.action_size)\n",
        "\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in memory\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def calculate_reward(self, predicted_label, actual_label, user_feedback=None):\n",
        "        \"\"\"Calculate reward based on prediction accuracy and user feedback\"\"\"\n",
        "        base_reward = 1.0 if predicted_label == actual_label else -1.0\n",
        "\n",
        "        # Bonus for user feedback\n",
        "        if user_feedback is not None:\n",
        "            if user_feedback == 'correct':\n",
        "                base_reward += 0.5\n",
        "            elif user_feedback == 'incorrect':\n",
        "                base_reward -= 0.5\n",
        "\n",
        "        return base_reward\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        \"\"\"Train the agent on a batch of experiences\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            if state not in self.q_table:\n",
        "                self.q_table[state] = np.zeros(self.action_size)\n",
        "            if next_state not in self.q_table:\n",
        "                self.q_table[next_state] = np.zeros(self.action_size)\n",
        "\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += 0.95 * np.amax(self.q_table[next_state])\n",
        "\n",
        "            self.q_table[state][action] = target\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the RL agent\"\"\"\n",
        "        agent_data = {\n",
        "            'q_table': {str(k): v.tolist() for k, v in self.q_table.items()},\n",
        "            'epsilon': self.epsilon,\n",
        "            'reward_history': self.reward_history\n",
        "        }\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(agent_data, f)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load the RL agent\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, 'r') as f:\n",
        "                agent_data = json.load(f)\n",
        "            self.q_table = {eval(k): np.array(v) for k, v in agent_data['q_table'].items()}\n",
        "            self.epsilon = agent_data.get('epsilon', self.epsilon)\n",
        "            self.reward_history = agent_data.get('reward_history', [])\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED MODEL CLASS\n",
        "# ============================================\n",
        "class EnhancedToxicityDetector:\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.rl_agent = ToxicityRLAgent()\n",
        "        self.threshold_dict = {category: 0.5 for category in TOXICITY_CATEGORIES.keys()}\n",
        "        self.performance_history = []\n",
        "\n",
        "    def create_enhanced_pipeline(self):\n",
        "        \"\"\"Create an ensemble pipeline with multiple models\"\"\"\n",
        "\n",
        "        # TF-IDF Vectorizer with enhanced parameters\n",
        "        tfidf = TfidfVectorizer(\n",
        "            max_features=100000,\n",
        "            ngram_range=(1, 3),  # Increased n-gram range\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            strip_accents='unicode',\n",
        "            analyzer='word',\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        # Ensemble of multiple classifiers\n",
        "        xgb_clf = XGBClassifier(\n",
        "            n_estimators=500,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            reg_alpha=0.1,\n",
        "            reg_lambda=0.1,\n",
        "            eval_metric=\"logloss\",\n",
        "            use_label_encoder=False,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        lr_clf = LogisticRegression(\n",
        "            C=1.0,\n",
        "            solver='liblinear',\n",
        "            random_state=42,\n",
        "            max_iter=1000\n",
        "        )\n",
        "\n",
        "        # Voting ensemble\n",
        "        ensemble = VotingClassifier([\n",
        "            ('xgb', xgb_clf),\n",
        "            ('lr', lr_clf)\n",
        "        ], voting='soft')\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('tfidf', tfidf),\n",
        "            ('scaler', StandardScaler(with_mean=False)),  # For sparse matrices\n",
        "            ('ensemble', ensemble)\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"Enhanced training with cross-validation and hyperparameter tuning\"\"\"\n",
        "        print(\"ðŸš€ Starting enhanced training...\")\n",
        "\n",
        "        if self.pipeline is None:\n",
        "            self.create_enhanced_pipeline()\n",
        "\n",
        "        # Cross-validation for robust performance estimation\n",
        "        cv_scores = []\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        # --- FIX STARTS HERE ---\n",
        "        # Pass the numpy array values to skf.split to ensure correct indexing\n",
        "        for fold, (train_indices, val_indices) in enumerate(skf.split(X_train.values, y_train.values)):\n",
        "            print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "            # Use the indices to access the original X_train and y_train Series/array\n",
        "            X_fold_train = X_train.iloc[train_indices]\n",
        "            y_fold_train = y_train.iloc[train_indices]\n",
        "            X_fold_val = X_train.iloc[val_indices]\n",
        "            y_fold_val = y_train.iloc[val_indices]\n",
        "            # --- FIX ENDS HERE ---\n",
        "\n",
        "            # Create a copy of pipeline for this fold\n",
        "            fold_pipeline = self.create_enhanced_pipeline()\n",
        "            fold_pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "            # Evaluate fold\n",
        "            fold_preds = fold_pipeline.predict_proba(X_fold_val)[:, 1]\n",
        "            fold_auc = roc_auc_score(y_fold_val, fold_preds)\n",
        "            cv_scores.append(fold_auc)\n",
        "            print(f\"Fold {fold + 1} AUC: {fold_auc:.4f}\")\n",
        "\n",
        "        print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
        "\n",
        "        # Train on full training set\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Optimize thresholds if validation set provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            self.optimize_thresholds(X_val, y_val)\n",
        "\n",
        "        print(\"âœ… Training completed!\")\n",
        "\n",
        "    def optimize_thresholds(self, X_val, y_val):\n",
        "        \"\"\"Optimize classification thresholds\"\"\"\n",
        "        print(\"ðŸŽ¯ Optimizing classification thresholds...\")\n",
        "\n",
        "        val_probs = self.pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        best_f1 = 0\n",
        "        best_threshold = 0.5\n",
        "\n",
        "        thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "        threshold_results = []\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            val_preds = (val_probs >= threshold).astype(int)\n",
        "            f1 = f1_score(y_val, val_preds)\n",
        "            precision = precision_score(y_val, val_preds)\n",
        "            recall = recall_score(y_val, val_preds)\n",
        "\n",
        "            threshold_results.append({\n",
        "                'threshold': threshold,\n",
        "                'f1': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall\n",
        "            })\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        # Update threshold for binary classification\n",
        "        self.threshold_dict['binary'] = best_threshold\n",
        "        print(f\"âœ… Best threshold: {best_threshold:.3f} with F1: {best_f1:.3f}\")\n",
        "\n",
        "        return threshold_results\n",
        "\n",
        "    def predict_with_rl(self, text, use_rl=True):\n",
        "        \"\"\"Make prediction with optional RL enhancement\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        # Clean and process text\n",
        "        cleaned_text = clean_text(text)\n",
        "        text_features = extract_features(cleaned_text)\n",
        "\n",
        "        # Get base prediction\n",
        "        prob = self.pipeline.predict_proba([cleaned_text])[0, 1]\n",
        "        base_prediction = int(prob >= self.threshold_dict.get('binary', 0.5))\n",
        "\n",
        "        raw_score = prob\n",
        "\n",
        "        if use_rl:\n",
        "            # Get RL agent's recommendation\n",
        "            state = self.rl_agent.get_state(text_features, prob)\n",
        "            rl_action = self.rl_agent.act(state)\n",
        "\n",
        "            # Combine base prediction with RL recommendation\n",
        "            # RL action 0 = not toxic, actions 1-5 = different toxicity types\n",
        "            rl_prediction = 1 if rl_action > 0 else 0\n",
        "\n",
        "            # Weighted combination\n",
        "            final_prediction = int(0.7 * base_prediction + 0.3 * rl_prediction >= 0.5)\n",
        "\n",
        "\n",
        "            return {\n",
        "                'text': text,\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'probability': float(prob),\n",
        "                'base_prediction': base_prediction,\n",
        "                'rl_action': int(rl_action),\n",
        "                'final_prediction': final_prediction,\n",
        "                'toxicity_type': list(TOXICITY_CATEGORIES.keys())[rl_action] if rl_action > 0 else 'clean',\n",
        "                'confidence': float(abs(prob - 0.5) * 2),  # Confidence metric\n",
        "                'features': text_features,\n",
        "                'rawScore': float(raw_score)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'text': text,\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'probability': float(prob),\n",
        "                'prediction': base_prediction,\n",
        "                'confidence': float(abs(prob - 0.5) * 2),\n",
        "                'features': text_features,\n",
        "                'rawScore': float(raw_score)\n",
        "            }\n",
        "\n",
        "    def learn_from_feedback(self, text, predicted_result, actual_label, user_feedback=None):\n",
        "        \"\"\"Learn from user feedback using RL\"\"\"\n",
        "        if 'rl_action' not in predicted_result:\n",
        "            return\n",
        "\n",
        "        text_features = extract_features(clean_text(text))\n",
        "        state = self.rl_agent.get_state(text_features, predicted_result['probability'])\n",
        "        action = predicted_result['rl_action']\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self.rl_agent.calculate_reward(\n",
        "            predicted_result['final_prediction'],\n",
        "            actual_label,\n",
        "            user_feedback\n",
        "        )\n",
        "\n",
        "        # Store experience (simplified - next_state is same as current state for this example)\n",
        "        self.rl_agent.remember(state, action, reward, state, True)\n",
        "        self.rl_agent.reward_history.append(reward)\n",
        "\n",
        "        # Train RL agent\n",
        "        self.rl_agent.replay()\n",
        "\n",
        "        print(f\"âœ… RL agent learned from feedback. Reward: {reward:.2f}\")\n",
        "\n",
        "    def save_model(self, model_path=MODEL_PATH):\n",
        "        \"\"\"Save the complete model\"\"\"\n",
        "        model_data = {\n",
        "            'pipeline': self.pipeline,\n",
        "            'threshold_dict': self.threshold_dict,\n",
        "            'performance_history': self.performance_history\n",
        "        }\n",
        "        joblib.dump(model_data, model_path)\n",
        "        self.rl_agent.save(RL_MODEL_PATH)\n",
        "        print(f\"âœ… Model saved to {model_path} and {RL_MODEL_PATH}\")\n",
        "\n",
        "    def load_model(self, model_path=MODEL_PATH):\n",
        "        \"\"\"Load the complete model\"\"\"\n",
        "        if os.path.exists(model_path):\n",
        "            model_data = joblib.load(model_path)\n",
        "            self.pipeline = model_data['pipeline']\n",
        "            self.threshold_dict = model_data['threshold_dict']\n",
        "            self.performance_history = model_data.get('performance_history', [])\n",
        "            self.rl_agent.load(RL_MODEL_PATH)\n",
        "            print(f\"âœ… Model loaded from {model_path}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED DATASET PREPARATION\n",
        "# ============================================\n",
        "def prepare_enhanced_dataset(df_path=\"jigsaw_data/train.csv\", sample_size=20000):\n",
        "    \"\"\"Prepare dataset with enhanced preprocessing and balancing\"\"\"\n",
        "    print(\"ðŸ“Š Preparing enhanced dataset...\")\n",
        "\n",
        "    df = pd.read_csv(df_path)\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # Create comprehensive toxicity label\n",
        "    toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # Calculate weighted toxicity score\n",
        "    df['toxicity_score'] = 0\n",
        "    for col in toxicity_cols:\n",
        "        if col in df.columns:\n",
        "            weight = TOXICITY_CATEGORIES.get(col, {}).get('weight', 1.0)\n",
        "            df['toxicity_score'] += df[col] * weight\n",
        "\n",
        "    # Binary label based on weighted score\n",
        "    df['label'] = (df['toxicity_score'] > 0).astype(int)\n",
        "\n",
        "    # Clean text\n",
        "    df['comment_text'] = df['comment_text'].apply(clean_text)\n",
        "\n",
        "    # Remove empty comments\n",
        "    df = df[df['comment_text'].str.len() > 0]\n",
        "\n",
        "    # Balanced sampling with stratification\n",
        "    toxic = df[df['label'] == 1]\n",
        "    clean = df[df['label'] == 0]\n",
        "\n",
        "    n_toxic = min(len(toxic), sample_size // 3)  # 1:2 ratio for better balance\n",
        "    n_clean = min(len(clean), sample_size - n_toxic)\n",
        "\n",
        "    df_balanced = pd.concat([\n",
        "        toxic.sample(n_toxic, random_state=42),\n",
        "        clean.sample(n_clean, random_state=42)\n",
        "    ]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Balanced dataset shape: {df_balanced.shape}\")\n",
        "    print(f\"Toxicity ratio: {df_balanced['label'].mean():.3f}\")\n",
        "\n",
        "    return df_balanced[['comment_text', 'label']]\n",
        "\n",
        "def main_training_pipeline():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    print(\"ðŸš€ Starting main training pipeline...\")\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(\"jigsaw_data/train.csv\"):\n",
        "        print(\"âŒ Dataset not found. Please download the Jigsaw dataset and place train.csv in 'jigsaw_data/' folder\")\n",
        "        print(\"You can download it from: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\")\n",
        "\n",
        "        # Create dummy dataset for demo\n",
        "        print(\"Creating dummy dataset for demonstration...\")\n",
        "        dummy_data = {\n",
        "            'comment_text': [\n",
        "                \"You're so stupid!\",\n",
        "                \"I hate you!\",\n",
        "                \"This is great work\",\n",
        "                \"Thank you for helping\",\n",
        "                \"You're worthless\",\n",
        "                \"Have a nice day\",\n",
        "                \"Go kill yourself\",\n",
        "                \"I love this project\",\n",
        "                \"You're an idiot\",\n",
        "                \"Well done!\"\n",
        "            ] * 200,  # Repeat to get more data\n",
        "            'label': [1, 1, 0, 0, 1, 0, 1, 0, 1, 0] * 200\n",
        "        }\n",
        "        df = pd.DataFrame(dummy_data)\n",
        "    else:\n",
        "        df = prepare_enhanced_dataset()\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        df['comment_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    print(f\"Training set size: {len(X_train)}\")\n",
        "    print(f\"Validation set size: {len(X_val)}\")\n",
        "\n",
        "    # Initialize and train model\n",
        "    detector = EnhancedToxicityDetector()\n",
        "    detector.train(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Save model\n",
        "    detector.save_model()\n",
        "\n",
        "    return detector\n",
        "\n",
        "# ============================================\n",
        "# BATCH PROCESSING & MONITORING\n",
        "# ============================================\n",
        "def batch_process_with_rl(detector, texts, batch_size=100):\n",
        "    \"\"\"Process texts in batches with RL enhancement\"\"\"\n",
        "    results = []\n",
        "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        print(f\"Processing batch {i//batch_size + 1}/{total_batches}...\")\n",
        "\n",
        "        for text in batch:\n",
        "            result = detector.predict_with_rl(text)\n",
        "            batch_results.append(result)\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Small delay to prevent overwhelming\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# ADVANCED FEATURES\n",
        "# ============================================\n",
        "def create_toxicity_report(detector, text):\n",
        "    \"\"\"Generate detailed toxicity analysis report\"\"\"\n",
        "    result = detector.predict_with_rl(text)\n",
        "\n",
        "    report = {\n",
        "        \"text\": text,\n",
        "        \"analysis\": {\n",
        "            \"overall_toxicity\": result['final_prediction'],\n",
        "            \"confidence\": result['confidence'],\n",
        "            \"probability\": result['probability'],\n",
        "            \"toxicity_type\": result['toxicity_type']\n",
        "        },\n",
        "        \"features\": result['features'],\n",
        "        \"risk_factors\": [],\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "\n",
        "    # Analyze risk factors\n",
        "    if result['features']['caps_ratio'] > 0.3:\n",
        "        report[\"risk_factors\"].append(\"High proportion of capital letters\")\n",
        "\n",
        "    if result['features']['exclamation_count'] > 2:\n",
        "        report[\"risk_factors\"].append(\"Multiple exclamation marks\")\n",
        "\n",
        "    if result['features']['profanity_indicators'] > 0:\n",
        "        report[\"risk_factors\"].append(\"Contains potentially offensive language\")\n",
        "\n",
        "    # Generate recommendations\n",
        "    if result['final_prediction']:\n",
        "        report[\"recommendations\"] = [\n",
        "            \"Consider reviewing this content before publication\",\n",
        "            \"May require moderation or content warning\",\n",
        "            \"Could benefit from rephrasing for more constructive communication\"\n",
        "        ]\n",
        "    else:\n",
        "        report[\"recommendations\"] = [\n",
        "            \"Content appears appropriate for publication\",\n",
        "            \"Maintains respectful communication standards\"\n",
        "        ]\n",
        "\n",
        "    return report\n",
        "\n",
        "def export_model_insights(detector, output_path=\"model_insights.json\"):\n",
        "    \"\"\"Export model insights and statistics\"\"\"\n",
        "    insights = {\n",
        "        \"model_info\": {\n",
        "            \"type\": \"Enhanced Toxicity Detector with Reinforcement Learning\",\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"features\": [\n",
        "                \"Ensemble classifier (XGBoost + Logistic Regression)\",\n",
        "                \"Advanced text preprocessing\",\n",
        "                \"Reinforcement learning for adaptive predictions\",\n",
        "                \"Multi-category toxicity detection\",\n",
        "                \"Dynamic threshold optimization\"\n",
        "            ]\n",
        "        },\n",
        "        \"rl_stats\": {\n",
        "            \"epsilon\": detector.rl_agent.epsilon,\n",
        "            \"memory_size\": len(detector.rl_agent.memory),\n",
        "            \"total_experiences\": len(detector.rl_agent.reward_history),\n",
        "            \"average_reward\": np.mean(detector.rl_agent.reward_history) if detector.rl_agent.reward_history else 0,\n",
        "            \"recent_performance\": np.mean(detector.rl_agent.reward_history[-100:]) if len(detector.rl_agent.reward_history) >= 100 else 0\n",
        "        },\n",
        "        \"thresholds\": detector.threshold_dict,\n",
        "        \"toxicity_categories\": TOXICITY_CATEGORIES\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(insights, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… Model insights exported to {output_path}\")\n",
        "\n",
        "# ============================================\n",
        "# TESTING & VALIDATION\n",
        "# ============================================\n",
        "def run_comprehensive_tests(detector):\n",
        "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
        "    print(\"ðŸ§ª Running comprehensive tests...\")\n",
        "\n",
        "    test_cases = [\n",
        "        # Clearly toxic\n",
        "        {\"text\": \"You're so stupid, I hate you!\", \"expected\": 1},\n",
        "        {\"text\": \"Kill yourself, nobody likes you\", \"expected\": 1},\n",
        "        {\"text\": \"You're worthless trash\", \"expected\": 1},\n",
        "\n",
        "        # Borderline cases\n",
        "        {\"text\": \"This is really annoying\", \"expected\": 0},\n",
        "        {\"text\": \"That's pretty dumb\", \"expected\": 0},\n",
        "        {\"text\": \"I disagree with your opinion\", \"expected\": 0},\n",
        "\n",
        "        # Clearly clean\n",
        "        {\"text\": \"Great job on the project!\", \"expected\": 0},\n",
        "        {\"text\": \"Thank you for your help\", \"expected\": 0},\n",
        "        {\"text\": \"Have a wonderful day\", \"expected\": 0},\n",
        "\n",
        "        # Edge cases\n",
        "        {\"text\": \"\", \"expected\": 0},\n",
        "        {\"text\": \"!!!!!!\", \"expected\": 0},\n",
        "        {\"text\": \"HELLO WORLD\", \"expected\": 0}\n",
        "    ]\n",
        "\n",
        "    correct_predictions = 0\n",
        "    results = []\n",
        "\n",
        "    for test_case in test_cases:\n",
        "        if not test_case[\"text\"]:  # Skip empty text\n",
        "            continue\n",
        "\n",
        "        result = detector.predict_with_rl(test_case[\"text\"])\n",
        "        prediction = result['final_prediction']\n",
        "        expected = test_case[\"expected\"]\n",
        "\n",
        "        is_correct = prediction == expected\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        results.append({\n",
        "            \"text\": test_case[\"text\"],\n",
        "            \"expected\": expected,\n",
        "            \"predicted\": prediction,\n",
        "            \"probability\": result['probability'],\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "\n",
        "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
        "        print(f\"{status} '{test_case['text']}' | Expected: {expected}, Got: {prediction} ({result['probability']:.3f})\")\n",
        "\n",
        "    accuracy = correct_predictions / len([tc for tc in test_cases if tc[\"text\"]])\n",
        "    print(f\"\\nðŸŽ¯ Test Accuracy: {accuracy:.2%} ({correct_predictions}/{len([tc for tc in test_cases if tc['text']])})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# USAGE EXAMPLES & DEMO\n",
        "# ============================================\n",
        "def demo_interactive_session(detector):\n",
        "    \"\"\"Interactive demo session\"\"\"\n",
        "    print(\"\\nðŸŽ® Interactive Toxicity Detection Demo\")\n",
        "    print(\"Type 'quit' to exit, 'help' for commands\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nEnter text to analyze: \").strip()\n",
        "\n",
        "            if user_input.lower() == 'quit':\n",
        "                break\n",
        "            elif user_input.lower() == 'help':\n",
        "                print(\"\"\"\n",
        "Commands:\n",
        "- Type any text to analyze toxicity\n",
        "- 'stats' - Show model statistics\n",
        "- 'report <text>' - Generate detailed report\n",
        "- 'quit' - Exit demo\n",
        "                \"\"\")\n",
        "                continue\n",
        "            elif user_input.lower() == 'stats':\n",
        "                stats = {\n",
        "                    'RL Epsilon': detector.rl_agent.epsilon,\n",
        "                    'Memory Size': len(detector.rl_agent.memory),\n",
        "                    'Total Feedback': len(detector.rl_agent.reward_history),\n",
        "                    'Avg Reward': np.mean(detector.rl_agent.reward_history[-50:]) if detector.rl_agent.reward_history else 0\n",
        "                }\n",
        "                for key, value in stats.items():\n",
        "                    print(f\"{key}: {value}\")\n",
        "                continue\n",
        "            elif user_input.lower().startswith('report '):\n",
        "                text = user_input[7:]  # Remove 'report ' prefix\n",
        "                report = create_toxicity_report(detector, text)\n",
        "                print(json.dumps(report, indent=2))\n",
        "                continue\n",
        "            elif not user_input:\n",
        "                print(\"Please enter some text to analyze.\")\n",
        "                continue\n",
        "\n",
        "            # Analyze the text\n",
        "            result = detector.predict_with_rl(user_input)\n",
        "\n",
        "            # Display results\n",
        "            status = \"ðŸš¨ TOXIC\" if result['final_prediction'] else \"âœ… CLEAN\"\n",
        "            print(f\"\\nResult: {status}\")\n",
        "            print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "            print(f\"Probability: {result['probability']:.3f}\")\n",
        "            print(f\"Type: {result['toxicity_type']}\")\n",
        "\n",
        "            # Ask for feedback\n",
        "            feedback = input(\"\\nWas this prediction correct? (y/n/skip): \").strip().lower()\n",
        "            if feedback in ['y', 'yes']:\n",
        "                detector.learn_from_feedback(user_input, result, result['final_prediction'], 'correct')\n",
        "                print(\"âœ… Thank you! The model learned from your feedback.\")\n",
        "            elif feedback in ['n', 'no']:\n",
        "                detector.learn_from_feedback(user_input, result, 1 - result['final_prediction'], 'incorrect')\n",
        "                print(\"âœ… Thank you! The model learned from your correction.\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nDemo interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# ============================================\n",
        "# FLASK WEB APPLICATION\n",
        "# ============================================\n",
        "\n",
        "# Initialize Flask app and detector globally\n",
        "app = Flask(__name__)\n",
        "detector = None\n",
        "\n",
        "# Dashboard HTML template\n",
        "DASHBOARD_TEMPLATE = '''\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Toxicity Detection Dashboard</title>\n",
        "    <style>\n",
        "        * {\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            box-sizing: border-box;\n",
        "        }\n",
        "\n",
        "        body {\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "            min-height: 100vh;\n",
        "            padding: 20px;\n",
        "        }\n",
        "\n",
        "        .container {\n",
        "            max-width: 1200px;\n",
        "            margin: 0 auto;\n",
        "            background: white;\n",
        "            border-radius: 15px;\n",
        "            box-shadow: 0 20px 40px rgba(0,0,0,0.1);\n",
        "            overflow: hidden;\n",
        "        }\n",
        "\n",
        "        .header {\n",
        "            background: linear-gradient(45deg, #667eea, #764ba2);\n",
        "            color: white;\n",
        "            padding: 30px;\n",
        "            text-align: center;\n",
        "        }\n",
        "\n",
        "        .header h1 {\n",
        "            font-size: 2.5rem;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "\n",
        "        .header p {\n",
        "            font-size: 1.1rem;\n",
        "            opacity: 0.9;\n",
        "        }\n",
        "\n",
        "        .content {\n",
        "            padding: 40px;\n",
        "        }\n",
        "\n",
        "        .stats-grid {\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "            gap: 20px;\n",
        "            margin-bottom: 40px;\n",
        "        }\n",
        "\n",
        "        .stat-card {\n",
        "            background: #f8f9ff;\n",
        "            border: 1px solid #e2e8f0;\n",
        "            border-radius: 10px;\n",
        "            padding: 25px;\n",
        "            text-align: center;\n",
        "            transition: transform 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .stat-card:hover {\n",
        "            transform: translateY(-5px);\n",
        "            box-shadow: 0 10px 25px rgba(0,0,0,0.1);\n",
        "        }\n",
        "\n",
        "        .stat-number {\n",
        "            font-size: 2rem;\n",
        "            font-weight: bold;\n",
        "            color: #667eea;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "\n",
        "        .stat-label {\n",
        "            color: #64748b;\n",
        "            font-weight: 500;\n",
        "        }\n",
        "\n",
        "        .test-section {\n",
        "            background: #f8f9ff;\n",
        "            border-radius: 10px;\n",
        "            padding: 30px;\n",
        "            margin-bottom: 30px;\n",
        "        }\n",
        "\n",
        "        .test-section h2 {\n",
        "            color: #1e293b;\n",
        "            margin-bottom: 20px;\n",
        "            font-size: 1.5rem;\n",
        "        }\n",
        "\n",
        "        .input-group {\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        .input-group label {\n",
        "            display: block;\n",
        "            margin-bottom: 8px;\n",
        "            color: #374151;\n",
        "            font-weight: 500;\n",
        "        }\n",
        "\n",
        "        .input-group textarea {\n",
        "            width: 100%;\n",
        "            padding: 15px;\n",
        "            border: 2px solid #e2e8f0;\n",
        "            border-radius: 8px;\n",
        "            font-size: 1rem;\n",
        "            resize: vertical;\n",
        "            min-height: 120px;\n",
        "            transition: border-color 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .input-group textarea:focus {\n",
        "            outline: none;\n",
        "            border-color: #667eea;\n",
        "            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);\n",
        "        }\n",
        "\n",
        "        .btn {\n",
        "            background: linear-gradient(45deg, #667eea, #764ba2);\n",
        "            color: white;\n",
        "            border: none;\n",
        "            padding: 15px 30px;\n",
        "            border-radius: 8px;\n",
        "            cursor: pointer;\n",
        "            font-size: 1rem;\n",
        "            font-weight: 500;\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .btn:hover {\n",
        "            transform: translateY(-2px);\n",
        "            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);\n",
        "        }\n",
        "\n",
        "        .result {\n",
        "            margin-top: 20px;\n",
        "            padding: 20px;\n",
        "            border-radius: 8px;\n",
        "            border-left: 4px solid;\n",
        "        }\n",
        "\n",
        "        .result.toxic {\n",
        "            background: #fef2f2;\n",
        "            border-color: #ef4444;\n",
        "            color: #991b1b;\n",
        "        }\n",
        "\n",
        "        .result.clean {\n",
        "            background: #f0fdf4;\n",
        "            border-color: #22c55e;\n",
        "            color: #166534;\n",
        "        }\n",
        "\n",
        "        .feature-list {\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
        "            gap: 15px;\n",
        "            margin-top: 15px;\n",
        "        }\n",
        "\n",
        "        .feature-item {\n",
        "            background: white;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            border: 1px solid #e2e8f0;\n",
        "        }\n",
        "\n",
        "        .feedback-section {\n",
        "            background: #fef3c7;\n",
        "            border-radius: 8px;\n",
        "            padding: 20px;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "\n",
        "        .feedback-buttons {\n",
        "            display: flex;\n",
        "            gap: 10px;\n",
        "            margin-top: 15px;\n",
        "        }\n",
        "\n",
        "        .btn-small {\n",
        "            padding: 8px 16px;\n",
        "            font-size: 0.9rem;\n",
        "        }\n",
        "\n",
        "        .btn-success {\n",
        "            background: #22c55e;\n",
        "        }\n",
        "\n",
        "        .btn-danger {\n",
        "            background: #ef4444;\n",
        "        }\n",
        "\n",
        "        .api-section {\n",
        "            background: #1e293b;\n",
        "            color: white;\n",
        "            border-radius: 10px;\n",
        "            padding: 30px;\n",
        "            margin-top: 30px;\n",
        "        }\n",
        "\n",
        "        .api-section h2 {\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        .api-endpoint {\n",
        "            background: #334155;\n",
        "            border-radius: 5px;\n",
        "            padding: 15px;\n",
        "            margin-bottom: 15px;\n",
        "            font-family: 'Courier New', monospace;\n",
        "        }\n",
        "\n",
        "        .method {\n",
        "            color: #22c55e;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "\n",
        "        .loading {\n",
        "            display: none;\n",
        "            text-align: center;\n",
        "            padding: 20px;\n",
        "        }\n",
        "\n",
        "        .spinner {\n",
        "            border: 4px solid #f3f4f6;\n",
        "            border-top: 4px solid #667eea;\n",
        "            border-radius: 50%;\n",
        "            width: 40px;\n",
        "            height: 40px;\n",
        "            animation: spin 1s linear infinite;\n",
        "            margin: 0 auto 10px;\n",
        "        }\n",
        "\n",
        "        @keyframes spin {\n",
        "            0% { transform: rotate(0deg); }\n",
        "            100% { transform: rotate(360deg); }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\">\n",
        "            <h1>ðŸ›¡ï¸ Toxicity Detection Dashboard</h1>\n",
        "            <p>Enhanced ML Model with Reinforcement Learning</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"content\">\n",
        "            <!-- Statistics Section -->\n",
        "            <div class=\"stats-grid\">\n",
        "                <div class=\"stat-card\">\n",
        "                    <div class=\"stat-number\" id=\"epsilon\">{{ stats.epsilon }}</div>\n",
        "                    <div class=\"stat-label\">RL Epsilon</div>\n",
        "                </div>\n",
        "                <div class=\"stat-card\">\n",
        "                    <div class=\"stat-number\" id=\"memory-size\">{{ stats.memory_size }}</div>\n",
        "                    <div class=\"stat-label\">Memory Size</div>\n",
        "                </div>\n",
        "                <div class=\"stat-card\">\n",
        "                    <div class=\"stat-number\" id=\"total-feedback\">{{ stats.total_feedback }}</div>\n",
        "                    <div class=\"stat-label\">Total Feedback</div>\n",
        "                </div>\n",
        "                <div class=\"stat-card\">\n",
        "                    <div class=\"stat-number\" id=\"avg-reward\">{{ \"%.3f\"|format(stats.avg_reward) }}</div>\n",
        "                    <div class=\"stat-label\">Avg Reward</div>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <!-- Test Section -->\n",
        "            <div class=\"test-section\">\n",
        "                <h2>ðŸ§ª Test the Model</h2>\n",
        "                <div class=\"input-group\">\n",
        "                    <label for=\"test-text\">Enter text to analyze:</label>\n",
        "                    <textarea id=\"test-text\" placeholder=\"Type your text here...\"></textarea>\n",
        "                </div>\n",
        "                <button class=\"btn\" onclick=\"analyzeText()\">Analyze Text</button>\n",
        "\n",
        "                <div class=\"loading\" id=\"loading\">\n",
        "                    <div class=\"spinner\"></div>\n",
        "                    <p>Analyzing text...</p>\n",
        "                </div>\n",
        "\n",
        "                <div id=\"result-container\"></div>\n",
        "            </div>\n",
        "\n",
        "            <!-- API Documentation -->\n",
        "            <div class=\"api-section\">\n",
        "                <h2>ðŸ“¡ API Endpoints</h2>\n",
        "                <div class=\"api-endpoint\">\n",
        "                    <span class=\"method\">POST</span> /api/predict\n",
        "                    <br><small>Analyze text for toxicity</small>\n",
        "                </div>\n",
        "                <div class=\"api-endpoint\">\n",
        "                    <span class=\"method\">POST</span> /api/feedback\n",
        "                    <br><small>Submit feedback for model improvement</small>\n",
        "                </div>\n",
        "                <div class=\"api-endpoint\">\n",
        "                    <span class=\"method\">GET</span> /api/stats\n",
        "                    <br><small>Get model statistics</small>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        async function analyzeText() {\n",
        "            const text = document.getElementById('test-text').value;\n",
        "            if (!text.trim()) {\n",
        "                alert('Please enter some text to analyze');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            const loading = document.getElementById('loading');\n",
        "            const resultContainer = document.getElementById('result-container');\n",
        "\n",
        "            loading.style.display = 'block';\n",
        "            resultContainer.innerHTML = '';\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/api/predict', {\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json',\n",
        "                    },\n",
        "                    body: JSON.stringify({ text: text })\n",
        "                });\n",
        "\n",
        "                const result = await response.json();\n",
        "                loading.style.display = 'none';\n",
        "\n",
        "                if (response.ok) {\n",
        "                    displayResult(result);\n",
        "                } else {\n",
        "                    resultContainer.innerHTML = `<div class=\"result toxic\">Error: ${result.error}</div>`;\n",
        "                }\n",
        "            } catch (error) {\n",
        "                loading.style.display = 'none';\n",
        "                resultContainer.innerHTML = `<div class=\"result toxic\">Network error: ${error.message}</div>`;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function displayResult(result) {\n",
        "            const container = document.getElementById('result-container');\n",
        "            const isToxic = result.final_prediction === 1;\n",
        "            const status = isToxic ? 'TOXIC ðŸš¨' : 'CLEAN âœ…';\n",
        "            const resultClass = isToxic ? 'toxic' : 'clean';\n",
        "\n",
        "            const featuresHtml = Object.entries(result.features).map(([key, value]) =>\n",
        "                `<div class=\"feature-item\"><strong>${key}:</strong> ${value}</div>`\n",
        "            ).join('');\n",
        "\n",
        "            container.innerHTML = `\n",
        "                <div class=\"result ${resultClass}\">\n",
        "                    <h3>Result: ${status}</h3>\n",
        "                    <p><strong>Confidence:</strong> ${(result.confidence * 100).toFixed(1)}%</p>\n",
        "                    <p><strong>Probability:</strong> ${(result.probability * 100).toFixed(1)}%</p>\n",
        "                    <p><strong>Toxicity Type:</strong> ${result.toxicity_type}</p>\n",
        "\n",
        "                    <div class=\"feedback-section\">\n",
        "                        <p><strong>Was this prediction correct?</strong></p>\n",
        "                        <div class=\"feedback-buttons\">\n",
        "                            <button class=\"btn btn-small btn-success\" onclick=\"submitFeedback('${result.text}', true)\">\n",
        "                                âœ… Correct\n",
        "                            </button>\n",
        "                            <button class=\"btn btn-small btn-danger\" onclick=\"submitFeedback('${result.text}', false)\">\n",
        "                                âŒ Incorrect\n",
        "                            </button>\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <h4>Text Features:</h4>\n",
        "                    <div class=\"feature-list\">\n",
        "                        ${featuresHtml}\n",
        "                    </div>\n",
        "                </div>\n",
        "            `;\n",
        "        }\n",
        "\n",
        "        async function submitFeedback(text, isCorrect) {\n",
        "            try {\n",
        "                const response = await fetch('/api/feedback', {\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json',\n",
        "                    },\n",
        "                    body: JSON.stringify({\n",
        "                        text: text,\n",
        "                        feedback: isCorrect ? 'correct' : 'incorrect'\n",
        "                    })\n",
        "                });\n",
        "\n",
        "                const result = await response.json();\n",
        "\n",
        "                if (response.ok) {\n",
        "                    alert('Thank you for your feedback! The model will learn from this.');\n",
        "                    // Refresh stats\n",
        "                    location.reload();\n",
        "                } else {\n",
        "                    alert('Error submitting feedback: ' + result.error);\n",
        "                }\n",
        "            } catch (error) {\n",
        "                alert('Network error: ' + error.message);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Auto-refresh stats every 30 seconds\n",
        "        setInterval(() => {\n",
        "            fetch('/api/stats')\n",
        "                .then(response => response.json())\n",
        "                .then(data => {\n",
        "                    document.getElementById('epsilon').textContent = data.epsilon.toFixed(3);\n",
        "                    document.getElementById('memory-size').textContent = data.memory_size;\n",
        "                    document.getElementById('total-feedback').textContent = data.total_feedback;\n",
        "                    document.getElementById('avg-reward').textContent = data.avg_reward.toFixed(3);\n",
        "                })\n",
        "                .catch(console.error);\n",
        "        }, 30000);\n",
        "\n",
        "        // Enable Enter key for text analysis\n",
        "        document.getElementById('test-text').addEventListener('keypress', function(e) {\n",
        "            if (e.key === 'Enter' && e.ctrlKey) {\n",
        "                analyzeText();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Flask routes\n",
        "@app.route('/')\n",
        "def index():\n",
        "    \"\"\"Home page with basic interface\"\"\"\n",
        "    return '''\n",
        "    <h1>ðŸ›¡ï¸ Enhanced Toxicity Detection API</h1>\n",
        "    <p>Welcome to the Enhanced Toxicity Detection System with Reinforcement Learning!</p>\n",
        "    <h2>Available Endpoints:</h2>\n",
        "    <ul>\n",
        "        <li><a href=\"/dashboard\">ðŸ“Š Dashboard</a> - Interactive monitoring dashboard</li>\n",
        "        <li><strong>POST /api/predict</strong> - Analyze text for toxicity</li>\n",
        "        <li><strong>POST /api/feedback</strong> - Submit feedback for model improvement</li>\n",
        "        <li><strong>GET /api/stats</strong> - Get model statistics</li>\n",
        "    </ul>\n",
        "    <h2>Quick Test:</h2>\n",
        "    <form action=\"/api/predict\" method=\"post\" style=\"margin: 20px 0;\">\n",
        "        <textarea name=\"text\" placeholder=\"Enter text to analyze...\" style=\"width: 400px; height: 100px;\"></textarea><br><br>\n",
        "        <input type=\"submit\" value=\"Analyze Text\" style=\"padding: 10px 20px; background: #667eea; color: white; border: none; border-radius: 5px;\">\n",
        "    </form>\n",
        "    '''\n",
        "\n",
        "@app.route('/dashboard')\n",
        "def dashboard():\n",
        "    \"\"\"Monitoring dashboard\"\"\"\n",
        "    global detector\n",
        "    if detector is None:\n",
        "        return jsonify({'error': 'Model not loaded'}), 500\n",
        "\n",
        "    # Get current statistics\n",
        "    stats = {\n",
        "        'epsilon': round(detector.rl_agent.epsilon, 3),\n",
        "        'memory_size': len(detector.rl_agent.memory),\n",
        "        'total_feedback': len(detector.rl_agent.reward_history),\n",
        "        'avg_reward': np.mean(detector.rl_agent.reward_history[-50:]) if detector.rl_agent.reward_history else 0\n",
        "    }\n",
        "\n",
        "    return render_template_string(DASHBOARD_TEMPLATE, stats=stats)\n",
        "\n",
        "@app.route('/api/predict', methods=['POST'])\n",
        "def predict():\n",
        "    \"\"\"Predict toxicity for given text\"\"\"\n",
        "    global detector\n",
        "    if detector is None:\n",
        "        return jsonify({'error': 'Model not loaded'}), 500\n",
        "\n",
        "    try:\n",
        "        if request.method == 'POST':\n",
        "            if request.is_json:\n",
        "                data = request.get_json()\n",
        "                text = data.get('text', '')\n",
        "            else:\n",
        "                text = request.form.get('text', '')\n",
        "\n",
        "            if not text:\n",
        "                return jsonify({'error': 'No text provided'}), 400\n",
        "\n",
        "            # Get prediction\n",
        "            result = detector.predict_with_rl(text)\n",
        "\n",
        "            return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/feedback', methods=['POST'])\n",
        "def feedback():\n",
        "    \"\"\"Submit feedback for model improvement\"\"\"\n",
        "    global detector\n",
        "    if detector is None:\n",
        "        return jsonify({'error': 'Model not loaded'}), 500\n",
        "\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        text = data.get('text', '')\n",
        "        user_feedback = data.get('feedback', '')\n",
        "\n",
        "        if not text or not user_feedback:\n",
        "            return jsonify({'error': 'Missing text or feedback'}), 400\n",
        "\n",
        "        # Get current prediction\n",
        "        result = detector.predict_with_rl(text)\n",
        "\n",
        "        # Determine actual label based on feedback\n",
        "        if user_feedback == 'correct':\n",
        "            actual_label = result['final_prediction']\n",
        "        elif user_feedback == 'incorrect':\n",
        "            actual_label = 1 - result['final_prediction']\n",
        "        else:\n",
        "            return jsonify({'error': 'Invalid feedback. Use \"correct\" or \"incorrect\"'}), 400\n",
        "\n",
        "        # Learn from feedback\n",
        "        detector.learn_from_feedback(text, result, actual_label, user_feedback)\n",
        "\n",
        "        return jsonify({\n",
        "            'message': 'Feedback received and processed',\n",
        "            'text': text,\n",
        "            'feedback': user_feedback,\n",
        "            'actual_label': actual_label\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/stats', methods=['GET'])\n",
        "def stats():\n",
        "    \"\"\"Get model statistics\"\"\"\n",
        "    global detector\n",
        "    if detector is None:\n",
        "        return jsonify({'error': 'Model not loaded'}), 500\n",
        "\n",
        "    try:\n",
        "        return jsonify({\n",
        "            'epsilon': detector.rl_agent.epsilon,\n",
        "            'memory_size': len(detector.rl_agent.memory),\n",
        "            'total_feedback': len(detector.rl_agent.reward_history),\n",
        "            'avg_reward': np.mean(detector.rl_agent.reward_history[-50:]) if detector.rl_agent.reward_history else 0,\n",
        "            'threshold_dict': detector.threshold_dict,\n",
        "            'model_type': 'Enhanced Toxicity Detector with RL'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/batch', methods=['POST'])\n",
        "def batch_predict():\n",
        "    \"\"\"Batch prediction for multiple texts\"\"\"\n",
        "    global detector\n",
        "    if detector is None:\n",
        "        return jsonify({'error': 'Model not loaded'}), 500\n",
        "\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        texts = data.get('texts', [])\n",
        "\n",
        "        if not texts or not isinstance(texts, list):\n",
        "            return jsonify({'error': 'Please provide a list of texts'}), 400\n",
        "\n",
        "        if len(texts) > 100:\n",
        "            return jsonify({'error': 'Maximum 100 texts per batch'}), 400\n",
        "\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            if text.strip():\n",
        "                result = detector.predict_with_rl(text)\n",
        "                results.append(result)\n",
        "\n",
        "        return jsonify({\n",
        "            'total_processed': len(results),\n",
        "            'results': results\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ›¡ï¸ Enhanced Toxicity Detection System\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check if we should run training or use existing model\n",
        "    choice = input(\"Choose option:\\n1. Train new model\\n2. Load existing model\\n3. Interactive demo\\n4. Start Flask server\\nEnter choice (1-4): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\nðŸš€ Starting training pipeline...\")\n",
        "        detector = main_training_pipeline()\n",
        "\n",
        "        # Run tests\n",
        "        run_comprehensive_tests(detector)\n",
        "\n",
        "        # Export insights\n",
        "        export_model_insights(detector)\n",
        "\n",
        "        print(f\"\\nâœ… Training completed successfully!\")\n",
        "        print(f\"Model saved to: {MODEL_PATH}\")\n",
        "        print(f\"RL Agent saved to: {RL_MODEL_PATH}\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        detector = EnhancedToxicityDetector()\n",
        "        if detector.load_model():\n",
        "            print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "            # Run some tests\n",
        "            test_results = run_comprehensive_tests(detector)\n",
        "\n",
        "            # Option for interactive demo\n",
        "            if input(\"\\nStart interactive demo? (y/n): \").lower() == 'y':\n",
        "                demo_interactive_session(detector)\n",
        "        else:\n",
        "            print(\"âŒ No existing model found. Please train a new model first.\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        detector = EnhancedToxicityDetector()\n",
        "        if detector.load_model():\n",
        "            demo_interactive_session(detector)\n",
        "        else:\n",
        "            print(\"âŒ No existing model found. Please train a new model first.\")\n",
        "\n",
        "    elif choice == \"4\":\n",
        "        if FLASK_AVAILABLE:\n",
        "            detector = EnhancedToxicityDetector()\n",
        "            if not detector.load_model():\n",
        "                print(\"âŒ No existing model found. Creating a dummy model for demonstration...\")\n",
        "                # Create a simple demo model\n",
        "                from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "                from sklearn.linear_model import LogisticRegression\n",
        "                from sklearn.pipeline import Pipeline\n",
        "\n",
        "                # Create dummy training data\n",
        "                dummy_texts = [\n",
        "                    \"You're so stupid!\", \"I hate you!\", \"This is great work\", \"Thank you for helping\",\n",
        "                    \"You're worthless\", \"Have a nice day\", \"Go kill yourself\", \"I love this project\"\n",
        "                ]\n",
        "                dummy_labels = [1, 1, 0, 0, 1, 0, 1, 0]\n",
        "\n",
        "                # Simple pipeline\n",
        "                detector.pipeline = Pipeline([\n",
        "                    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
        "                    ('clf', LogisticRegression())\n",
        "                ])\n",
        "\n",
        "                detector.pipeline.fit(dummy_texts, dummy_labels)\n",
        "                detector.threshold_dict['binary'] = 0.5\n",
        "                print(\"âœ… Demo model created!\")\n",
        "\n",
        "            # Set up ngrok tunnel\n",
        "            port = 5000\n",
        "\n",
        "            try:\n",
        "                # You must get your own authtoken from ngrok.com\n",
        "                ngrok.set_auth_token(\"2igT3TPuLOjBYXVnG35DPjod9He_7A9pbGAqNiDofSiX9hh5N\")\n",
        "                public_url = ngrok.connect(port).public_url\n",
        "\n",
        "                print(\"ðŸŒ Starting Flask server...\")\n",
        "                print(\"Access the web interface at:\", public_url)\n",
        "                print(\"Local access:\", f\"http://127.0.0.1:{port}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Ngrok setup failed: {e}\")\n",
        "                print(\"ðŸŒ Starting Flask server locally...\")\n",
        "                print(\"Local access:\", f\"http://127.0.0.1:{port}\")\n",
        "\n",
        "            print(\"API endpoints:\")\n",
        "            print(\"  - GET / - Home page\")\n",
        "            print(\"  - GET /dashboard - Monitoring dashboard\")\n",
        "            print(\"  - POST /api/predict - Analyze text\")\n",
        "            print(\"  - POST /api/feedback - Submit feedback\")\n",
        "            print(\"  - GET /api/stats - Model statistics\")\n",
        "            print(\"  - POST /api/batch - Batch analysis\")\n",
        "\n",
        "            app.run(host='0.0.0.0', port=port, debug=True, use_reloader=False)\n",
        "        else:\n",
        "            print(\"âŒ Flask not available. Please install Flask: pip install flask pyngrok\")\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice. Please run the script again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsjFQGxD_MBJ",
        "outputId": "96620d0f-fddd-4f60-f6d0-a90dc28ffeb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ›¡ï¸ Enhanced Toxicity Detection System\n",
            "==================================================\n",
            "Choose option:\n",
            "1. Train new model\n",
            "2. Load existing model\n",
            "3. Interactive demo\n",
            "4. Start Flask server\n",
            "Enter choice (1-4): 3\n",
            "âœ… Model loaded from enhanced_toxicity_model.pkl\n",
            "\n",
            "ðŸŽ® Interactive Toxicity Detection Demo\n",
            "Type 'quit' to exit, 'help' for commands\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter text to analyze: What is this Ram?\n",
            "\n",
            "Result: ðŸš¨ TOXIC\n",
            "Confidence: 0.103\n",
            "Probability: 0.448\n",
            "Type: insult\n",
            "\n",
            "Was this prediction correct? (y/n/skip): y\n",
            "âœ… RL agent learned from feedback. Reward: 1.50\n",
            "âœ… Thank you! The model learned from your feedback.\n",
            "\n",
            "Enter text to analyze: quit\n"
          ]
        }
      ]
    }
  ]
}